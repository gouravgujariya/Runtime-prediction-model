Runtime Prediction Model
Gourav gujariya,Dr kuldeep Kumar
Dr B R Ambedkar national institute of technology Jalandhar
{gouravg.cs.20 ,kumark}@nitj.ac.in 
________________


Abstract. 
To avoid time consumption in the calculation of time complexity for algorithms on the internet, an automated tool using machine learning-based classification techniques is proposed in the paper. We used different classifiers for calculating the time complexity of algorithms based on features of algorithms and the results of those classifiers were compared for identifying the best Runtime prediction model. It helps in computing the time complexity from an enormous number of algorithms. The Runtime prediction model considers two types of classifiers, single and ensemble. Experiments show that ensemble classification is the best for detecting scam over single classification.
Keywords - time complexity, ensemble models, machine learning, runtime prediction.


Introduction 
In the study of design and analysis of algorithms, scientists are continuously developing algorithms for solving the problem statements such as traveling salesman, graph coloring, the sum of subsets, and Hamiltonian but for creating algorithms we also check algorithms for their effectiveness in the regards of past algorithms, throughout the world, there are approximately 4.5 million lines of code on GitHub, 2 billion lines of code in Google services and even 100 million lines of code contained in a modern vehicle and how can we say that the following code is more effective than previous algorithms by making the algorithm which takes less time of getting executed here execution term is based on runtime which shows the time taken based on input and is not based on hardware requirements. Because time complexity is calculated by using the big o method in which the selecting of the worst-case scenario is done to input and calculate time complexity, which is done manually and is hard to do. By The usage of artificial intelligence and data science, we can create a model which can do the same but with more effectiveness and within a fraction of seconds. The following model is based on a dataset[1].
1. Runtime prediction is one of the major issues in recent times addressed in the domain of Design and analysis of algorithms (DAA) [1]. In the recent period, many researchers prefer to compute their algorithm’s time complexity manually so these can be accessed correctly but time-consuming the researchers. However, this intention may be caused by one type of error due to human error and will be costing a loss of high-value time. This Run time prediction model draws good attention for obtaining an automated tool for computing time complexity and reporting them to researchers to avoid time consumption and getting an assumption about how effective their algorithms are. For this purpose, a machine learning approach is applied which uses several classification algorithms for recognizing patterns in features of algorithms. A classifier maps input variables to target classes by considering training data. Classifiers addressed in the paper for predicting runtime are described briefly. This classifier-based prediction may be broadly categorized into -Single Classifier based Prediction and Ensemble Classifiers based Prediction. 
   1. Single Classifier-based PredictionClassifiers are trained for predicting the unknown test cases. The following classifiers are used while predicting runtime complexity.
      1.  Support vector machines: A support vector machine or SVM is a supervised learning algorithm, which is used for classification as well as regression problems. The goal of the SVM algorithm is to create the best line or decision boundary that can separate the dimensional space into classes so that we can easily put the new data point in the correct category in the future (hyperplane).
      2. Random forest classifier: Random forest classifier, is a popular machine learning algorithm that belongs to the supervised learning technique. This classifier contains many decision trees on various subsets of the given dataset and takes the average to improve the predictive accuracy of that dataset. The greater number of trees present in the forest leads to higher accuracy of the model and prevents the problem of overfitting. 
      3. K-nearest Neighbor Classifier: K-Nearest Neighbour Classifiers [5], often known as lazy learners, identifies objects based on the closest proximity of training examples in the feature space. The classifier considers the k number of objects as the nearest object while determining the class. The main challenge of this classification technique relies on choosing the appropriate value of k [5].
A. Implementation of Classifiers 
In this framework, classifiers are trained using appropriate parameters. For maximizing the performance of these models, default parameters may not be sufficient enough. Adjustment of these parameters enhances the reliability of this model. The K-NN classifier gives a promising result for the value k=5 considering all the evaluating metrics. The support vector classifier also provides promising results with default parameters On the other hand, ensemble classifiers, such as Random Forest, AdaBoost bagging,lightGBM, and extra tree classifier are built based on 500 numbers of estimators on which the boosting is terminated. After constructing these classification models, training data are fitted into it. Later the testing dataset is used for prediction purposes. After the prediction is done, the performance of the classifiers is evaluated based on the predicted value and the actual value.
B. Performance Evaluation Metrics
 While evaluating the performance skill of a model, it is necessary to employ some metrics to justify the evaluation. For this purpose, the following metrics are taken into consideration to identify the best relevant problem-solving approach. Accuracy is a metric that identifies the ratio of true predictions over the total number of instances considered. However, the accuracy may not be enough metric for evaluating the model‘s performance since it does not consider wrong predicted cases. For measuring this compensation, precision and recall are quite necessary to be considered . Precision identifies the ratio of correct positive results over the number of positive results predicted by the classifier. Recall denotes the number of correct positive results divided by the number of all relevant samples. F1-Score or F-measure is a parameter that is concerned with both recall and precision and it is calculated as the harmonic mean of precision and recall higher values of accuracy, F1-Score signifies a better performing model.  
EXPERIMENTAL RESULTS 
All the above-mentioned classifiers are trained and tested for the prediction of time complexity over a given dataset. The following Table 1 shows the comparative study of the classifiers concerning evaluating metrics and Table 2 provides results for the classifiers that are based on ensemble techniques. Fig. 4 to Fig. 7 depict the overall performance of all the classifiers in terms of accuracy and f1-score.


B. Ensemble Approach based Classifiers Based on the facilitated results of the above classifiers we are using the ensemble approach because random forest outperforms all the simple machine learning algorithms. After all, the Ensemble approach facilitates several machine learning algorithms to perform together to obtain higher accuracy of the entire system.
1. Random forest (RF) exploits the concept of ensemble learning approach and regression technique applicable for classification-based problems. This classifier assimilates several tree-like classifiers which are applied to various sub-samples of the dataset and each tree casts its vote on the most appropriate class for the input. 
2. Extra Tree Classifier: Extremely randomized trees classifier(extra tree classifier) is a type of ensemble learning technique that aggregates the results of multiple decor-related decision trees collected in a forest to output its classification result. uses averaging to improve the predictive accuracy and control over-fitting.
3. Boosting is an efficient technique where several unstable learners are assimilated into a single learner to improve the accuracy of classification . 
4. AdaBoost is a good example of boosting technique that produces improved output even when the performance of the weak learners is inadequate.
5. Bagging is an ensemble meta-estimator that fits the base classifier each on random subsets of the original dataset and then aggregates their prediction(either by voting or by averaging)
6. LightGBM is a gradient boosting framework that uses tree-based learning algorithms. It is designed to be distributed and efficient with Faster training speed and higher efficiency.
7. Stacking :Stacking is an ensemble machine learning algorithm that masters how to best combine the predictions from numerous well-operating machine learning models.
8. Voting In classification problems, there are two types of voting: hard voting and soft voting. Hard voting entails picking the prediction with the highest number of votes, whereas soft voting entails combining the probabilities of each prediction in each model and picking the prediction with the highest total probability.


2 Related Work
In recent years, there has been extensive research in the deep learning community on programming codes. Hutter et al. [9] proposed supervised learning methods for algorithm runtime prediction. However, as explained before, execution time is not a standard measure to analyse efficiency of algorithms. Therefore, in our work, we do not consider algorithms’ execution times. Most of the research in deep learning has been focused on two buckets, either on predicting some structure/attribute in the program or generating code snippets that are syntactically and/or semantically correct.
Variable/Method name prediction is a widely attempted problem, wherein Allamanis et al. [3] used a convolutional neural network with attention technique to predict method names, Alon et al. [4] suggested the use of AST paths to be used as context for generating code embeddings and training classifiers on top of them. Yonai et al. [17] used call graphs to compute method embeddings and recommend names of existing methods with function similar to target function.
Another popular prediction problem is that of defect prediction, given a piece of code. Li et al. [11] used Abstract Syntax Trees of programs in their CNN for feature generation which were then used for defect prediction. A major goal in all these approaches is to come up with a representation of the source program, which effectively captures the syntactic and semantic features of the program. Chen and Monperrus [8] performed a survey on word embedding techniques used on source codes. However, so far, there has been no such work for predicting time complexity of programs using code embeddings. We have established the same as one of our baselines using graph2vec [13].
Srikant and Aggarwal [14] extract hand-engineered features from Control Flow and Data Dependency graphs of programs such as number of nested loops, number of instances of if statements in a loop etc. for automatic grading of programs. They then used the grading criteria, that correct test programs would have similar programming constructs/features as those in the correct hand-graded programs. We use the same idea of identifying key features as the other baseline, which are constructs that a human evaluator would look at, to compute complexity and use them to train the classification models. Though, unlike [14], our features are problem independent. Moreover, the solution in [14] is commercially deployed, and thus, their dataset is not publicly available.
Proposed Methodology
The target of this study is to predict the time complexity of the algorithm based on the features present in it. Providing an approximate answer for the prediction of the runtime will be helpful in the research of new algorithms. In this context, a dataset from MIDAS research concord-dataset is used that provides information regarding features in the algorithm that is the algorithm is containing the following features what is the time complexity of it. The dataset has the features as shown.


1. No_of_ifs:no of if presents in algorithms
2. no_of_switches: no of switches present in algorithm
3. no_of_loop: no of loops present in a algorithm
4. no_of_break: no of the breaks in an algorithm
5. priority_queue_present: if priority is present or not
6. no_of_sort:no of sort present
7. hash_set_present:hash set is present or not in algorithm
8. hash_map_present:hash map present or not
9. recursion_present:recursion present or not
10. nested_loop_depth:nested loop present or not
11. noOfVariables:no of variables present in algorithm
12. noOfMethods:no of methods used
13. noOfJumps:no of jumps done between statements in algorithm
14. noOfStatements:no of statement in an algorithm
15. complexity:time complexity of the algorithm (target feature)
16. file_name:filename to which algorithm is related


This dataset contains 934 algorithmic features. Which is used for testing the overall performance and learning of the approach. For a better understanding of the target feature as a baseline, a multi-step procedure is followed for obtaining a balanced dataset. Some pre-processing techniques are applied to this dataset before fitting this data to any classifier. Pre-processing techniques include label encoding, missing values removal, irrelevant attribute elimination, and feature scaling. This prepares the dataset to be transformed to obtain a feature vector. These feature vectors are fitted to several classifiers. 


A couple of classifiers are employed such as Random forest Classifier, support vector Classifier, K-nearest Neighbor Classifier, AdaBoost Classifier, LGMB Classifier, bagging, Extra tree classifier, the voting classifier(hard and soft), and Stacking Classifier for prediction of runtime complexity. It is to be noted that the complexity of the dataset is kept as the target class for classification purposes. At first, the classifiers are trained using 80% of the entire dataset and later 20% of the entire dataset is used for the prediction purpose. The performance measure metrics such as Accuracy and f1 score are used for evaluating the prediction for each of these classifiers. Finally, the classifier that has the best performance concerning all the metrics is chosen as the best candidate model.


A. Implementation of Classifiers 


In this framework, classifiers are trained using appropriate parameters. For maximizing the performance of these models, default parameters may not be sufficient enough. Adjustment of these parameters enhances the reliability of this model. The K-NN classifier gives a promising result for the value k=5 considering all the evaluating metrics. The support vector classifier also provides promising results with default parameters On the other hand, ensemble classifiers, such as Random Forest, AdaBoost bagging,lightGBM, and extra tree classifier are built based on 500 numbers of estimators on which the boosting is terminated. After constructing these classification models, training data are fitted into it. Later the testing dataset is used for prediction purposes. After the prediction is done, the performance of the classifiers is evaluated based on the predicted value and the actual value.


 B. Performance Evaluation Metrics


 While evaluating the performance skill of a model, it is necessary to employ some metrics to justify the evaluation. For this purpose, the following metrics are taken into consideration to identify the best relevant problem-solving approach. Accuracy is a metric that identifies the ratio of true predictions over the total number of instances considered. However, the accuracy may not be enough metric for evaluating the model‘s performance since it does not consider wrong predicted cases. For measuring this compensation, precision and recall are quite necessary to be considered . Precision identifies the ratio of correct positive results over the number of positive results predicted by the classifier. Recall  denotes the number of correct positive results divided by the number of all relevant samples. F1-Score or F-measure  is a parameter that is concerned with both recall and precision and it is calculated as the harmonic mean of precision and recall higher values of accuracy, F1-Score signifies a better performing model.  


IV. EXPERIMENTAL RESULTS 


All the above-mentioned classifiers are trained and tested for the prediction of time complexity over a given dataset. The following Table 1 shows the comparative study of the classifiers concerning evaluating metrics and Table 2 provides results for the classifiers that are based on ensemble techniques. Fig. 4 to Fig. 7 depict the overall performance of all the classifiers in terms of accuracy and f1-score.


  



index
	CLASSIFIER
	ACCURACY
	1
	KNN
	0.6928
	2
	RANDOM FOREST CLASSIFIER
	0.725
	3
	SUPPORT VECTOR MACHINE
	0.6821
	4
	LIGHT BM
	0.6928
	5
	ADABOOSTING
	0.6964
	6
	BAGGING
	0.7464
	7
	EXTRA TREE CLASSIFIER
	0.7428
	8
	STACKING
	0.7178
	9
	VOTING HARD
	0.7214
	10
	VOTING SOFT
	0.7178
	

F1 score:
* Knearest neighbour [0.85714286 0.56       0.72307692 0.59649123 0.5974026 ]
* Random forest classifier [0.79012346 0.54545455 0.77220077 0.6779661  0.625     ]
* support vector machine [0.63888889 0.375      0.70967742 0.71544715 0.62857143]
* light gbm [0.7654321  0.48       0.74590164 0.68907563 0.54945055]
* adaboost [0.7654321  0.5        0.73880597 0.64150943 0.61728395]
* bagging [0.80487805 0.6        0.79087452 0.69491525 0.64935065]
* extra tree classifier [0.79012346 0.66666667 0.77735849 0.70689655 0.64935065]
* stacking  [0.78481013 0.57142857 0.76981132 0.64957265 0.61538462]
* voting hard [0.81927711 0.52173913 0.76335878 0.6440678  0.64864865]
* voting soft [0.775      0.54545455 0.77394636 0.66666667 0.6       ]
From the above graph we can see that classifiers like Random forest, stacking , voting hard, extra tre classifier,voting soft, bagging outperform knn,Lgbm,ADAboost,SVM and we can say that these classifiers are not suitable for the prediction so for further analysis we take a look at f1 score classifiers among Random forest classifier, st ch cs bag, etc some classifiers like voting soft is classifying some feature very properly but is not classifying some feature with .54 f1 score is not suitable for selection similarly voting hard,stacking, Random forest having the same problem but among bagging an extra tree classifier we can see that extra tree classifier is outperforming and is classifying all features more efficiently
V. CONCLUSIONS 
Runtime complexity prediction will help the researchers in getting an idea that how their algorithm is performing and could save their time of computation. For the prediction of time complexity, several machine learning algorithms are proposed as countermeasures in this paper. The supervised mechanism is used to exemplify the use of several classifiers for employment scam detection. Experimental results indicate that the extra tree classifier outperforms its peer classification tool. The proposed approach achieved an accuracy of 74.28% which is much higher than the existing methods.


REFERENCES 
1. Learning Based Methods for Code Runtime Complexity Prediction | SpringerLink
2. 2105.12655v2.pdf (arxiv.org)
3. AI for Code: Predict Code complexity using IBM’s CodeNet Dataset
4. GitHub - midas-research/corcod-dataset: CoRCoD: Code Runtime Complexity Dataset
5. https://stats.stackexchange.com/questions/538891/can-we-make-bagging-equivalent-to-random-forest-if-we-take-max-features-in-our-o
6. https://lightgbm.readthedocs.io/
7. https://www.researchgate.net/profile/Samir-Bandyopadhyay/publication/341325717_Fake_Job_Recruitment_Detection_Using_Machine_Learning_Approach/links/5ebad816299bf1c09ab91341/Fake-Job-Recruitment-Detection-Using-Machine-Learning-Approach.pdf
8.